{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "IMDB_bidirectional_lstm.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3fYrfjj0zNE"
      },
      "source": [
        "# Bidirection LSTM - IMDB sentiment classification\n",
        "\n",
        "see **https://github.com/keras-team/keras/blob/master/examples/imdb_bidirectional_lstm.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bDZsqMr0zNH"
      },
      "source": [
        "KERAS_MODEL_FILEPATH = '../../demos/data/imdb_bidirectional_lstm/imdb_bidirectional_lstm.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uFZKUue0zNQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f263e744-31bb-41e5-bffd-c57bfe39efd8"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, Bidirectional\n",
        "from keras.datasets import imdb\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsgnUxAw0zNX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ebf5e37c-85d8-43a1-931d-bdafbfd78134"
      },
      "source": [
        "max_features = 20000\n",
        "maxlen = 200  # cut texts after this number of words (among top max_features most common words)\n",
        "\n",
        "print('Loading data...')\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(X_train), 'train sequences')\n",
        "print(len(X_test), 'test sequences')\n",
        "\n",
        "print(\"Pad sequences (samples x time)\")\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "X_train shape: (25000, 200)\n",
            "X_test shape: (25000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0uE6vhQ0zNc"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 64, input_length=maxlen))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3XfBrP70zNg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "c8cfe19b-a676-48ca-e60b-9b92af85852f"
      },
      "source": [
        "# Model saving callback\n",
        "checkpointer = ModelCheckpoint(filepath=KERAS_MODEL_FILEPATH, monitor='val_acc', verbose=1, save_best_only=True)\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_acc', verbose=1, patience=2)\n",
        "\n",
        "# train\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "model.fit(X_train, y_train, \n",
        "          validation_data=[X_test, y_test],\n",
        "          batch_size=batch_size, epochs=epochs, verbose=2,\n",
        "          callbacks=[checkpointer, early_stopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            " - 80s - loss: 0.4698 - accuracy: 0.7723 - val_loss: 0.3175 - val_accuracy: 0.8683\n",
            "Epoch 2/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " - 78s - loss: 0.2313 - accuracy: 0.9166 - val_loss: 0.3111 - val_accuracy: 0.8676\n",
            "Epoch 3/10\n",
            " - 77s - loss: 0.1567 - accuracy: 0.9495 - val_loss: 0.3727 - val_accuracy: 0.8626\n",
            "Epoch 4/10\n",
            " - 78s - loss: 0.1030 - accuracy: 0.9677 - val_loss: 0.4567 - val_accuracy: 0.8575\n",
            "Epoch 5/10\n",
            " - 78s - loss: 0.0802 - accuracy: 0.9758 - val_loss: 0.4204 - val_accuracy: 0.8608\n",
            "Epoch 6/10\n",
            " - 77s - loss: 0.0562 - accuracy: 0.9838 - val_loss: 0.5201 - val_accuracy: 0.8518\n",
            "Epoch 7/10\n",
            " - 78s - loss: 0.0558 - accuracy: 0.9837 - val_loss: 0.5900 - val_accuracy: 0.8512\n",
            "Epoch 8/10\n",
            " - 77s - loss: 0.0614 - accuracy: 0.9802 - val_loss: 0.6170 - val_accuracy: 0.8478\n",
            "Epoch 9/10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "jsRAIu850zNm"
      },
      "source": [
        "**sample data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNVVBn5W0zNn"
      },
      "source": [
        "word_index = imdb.get_word_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aYZDFNg0zNq"
      },
      "source": [
        "word_dict = {idx: word for word, idx in word_index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkWXNvGn0zNv"
      },
      "source": [
        "sample = []\n",
        "for idx in X_train[0]:\n",
        "    if idx >= 3:\n",
        "        sample.append(word_dict[idx-3])\n",
        "    elif idx == 2:\n",
        "        sample.append('-')\n",
        "' '.join(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS8Yd94v0zNz"
      },
      "source": [
        "with open('../../demos/data/imdb_bidirectional_lstm/imdb_dataset_word_index_top20000.json', 'w') as f:\n",
        "    f.write(json.dumps({word: idx for word, idx in word_index.items() if idx < max_features}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZId34c90zN2"
      },
      "source": [
        "with open('../../demos/data/imdb_bidirectional_lstm/imdb_dataset_word_dict_top20000.json', 'w') as f:\n",
        "    f.write(json.dumps({idx: word for word, idx in word_index.items() if idx < max_features}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dX8lZon0zN5"
      },
      "source": [
        "sample_test_data = []\n",
        "for i in np.random.choice(range(X_test.shape[0]), size=1000, replace=False):\n",
        "    sample_test_data.append({'values': X_test[i].tolist(), 'label': y_test[i].tolist()})\n",
        "    \n",
        "with open('../../demos/data/imdb_bidirectional_lstm/imdb_dataset_test.json', 'w') as f:\n",
        "    f.write(json.dumps(sample_test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6i46Fl10zN-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}